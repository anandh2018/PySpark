# app_config.yaml - Main application configuration

application:
  name: "CFA_SF_STRUCS_PARENT_REPORT"
  version: "1.0.0"
  environment: "production"  # development, staging, production

spark:
  master: "yarn"
  deploy_mode: "cluster"
  configs:
    # Core Spark Settings
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"
    spark.sql.adaptive.skewJoin.enabled: "true"
    spark.sql.shuffle.partitions: "200"
    
    # S3 Configuration
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.hadoop.fs.s3a.aws.credentials.provider: "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
    spark.hadoop.fs.s3a.endpoint: "s3.us-east-1.amazonaws.com"
    spark.hadoop.fs.s3a.path.style.access: "false"
    spark.hadoop.fs.s3a.block.size: "134217728"
    spark.hadoop.fs.s3a.multipart.size: "67108864"
    spark.hadoop.fs.s3a.fast.upload: "true"
    spark.hadoop.fs.s3a.fast.upload.buffer: "disk"
    spark.hadoop.fs.s3a.connection.maximum: "50"
    spark.hadoop.fs.s3a.connection.timeout: "600000"
    
    # Memory Settings
    spark.executor.memory: "8g"
    spark.driver.memory: "4g"
    spark.executor.memoryOverhead: "2g"
    spark.driver.memoryOverhead: "1g"
    
    # Parquet Optimization
    spark.sql.parquet.filterPushdown: "true"
    spark.sql.parquet.mergeSchema: "false"
    spark.sql.parquet.compression.codec: "snappy"
    spark.sql.files.maxPartitionBytes: "134217728"
    
    # Dynamic Allocation
    spark.dynamicAllocation.enabled: "true"
    spark.dynamicAllocation.minExecutors: "2"
    spark.dynamicAllocation.maxExecutors: "10"

processing:
  # Layer configurations
  layers:
    bronze:
      description: "Raw Oracle data from S3"
      enabled: true
    silver:
      description: "Staging/Transformed data"
      enabled: true
    gold:
      description: "Final aggregated reports"
      enabled: true
  
  # Execution steps
  execution_order:
    - load_bronze_tables
    - execute_silver_transformations
    - execute_gold_reports
  
  # Output settings
  output:
    format: "parquet"  # parquet, delta, csv
    compression: "snappy"
    write_mode: "overwrite"
    partition_by_date: true
    coalesce_partitions: 4
    
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_to_file: false
  log_file_path: "/var/log/cfa_sf_strucs_parent.log"
